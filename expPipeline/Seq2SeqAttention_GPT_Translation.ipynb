{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45de5f45-94f5-4731-8298-d91de8d94e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl (587.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /opt/conda/lib/python3.10/site-packages (from en-core-web-lg==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.10.9)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torchtext==0.15.2\n",
    "!pip install -q spacy\n",
    "!pip install -q indic-nlp-library\n",
    "!python3 -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921f29df-5845-4e14-84b3-61fd2451a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import io, gc\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.hi.examples import sentences\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer\n",
    "from torch import Tensor\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "#spacy.prefer_gpu() use only during multiple GPUs to avoid memory consumption "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b6b6b66-ccc6-422c-a0a6-c3abb8192fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jul 29 19:18:21 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P0    41W / 300W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b73258c1-d3aa-4321-aa04-622c74a38489",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('gs://bilingualdata/SHORT_HI_EN.csv', encoding='utf-8', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc6f737-4b7b-4672-98d2-7f9a03c5cacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127607, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db795e42-a5f1-4030-8659-10cd63417717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ted</td>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ted</td>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ted</td>\n",
       "      <td>what we really mean is that they're bad at not...</td>\n",
       "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>.The ending portion of these Vedas is called U...</td>\n",
       "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source                                   english_sentence  \\\n",
       "0        ted  politicians do not have permission to do what ...   \n",
       "1        ted         I'd like to tell you about one such child,   \n",
       "2  indic2012  This percentage is even greater than the perce...   \n",
       "3        ted  what we really mean is that they're bad at not...   \n",
       "4  indic2012  .The ending portion of these Vedas is called U...   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
       "1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
       "2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
       "3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
       "4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42c6f675-f95b-4d86-b63c-30fc947a50a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tides        50000\n",
       "ted          39881\n",
       "indic2012    37726\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8b6f74e-6534-4cf0-876f-70bd222a1d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('politicians do not have permission to do what needs to be done.',\n",
       " 'राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह करने कि अनुमति नहीं है .')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['english_sentence'][0], data['hindi_sentence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6d406f5-25e7-4e99-a17a-986947c1c651",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input_df = data['english_sentence'].tolist()\n",
    "user_output_df = data['hindi_sentence'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59e00768-d1d5-469b-9727-6c1fb3d4c756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127607, 127607)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_input_df), len(user_output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e4c6a63-5439-4e66-a6fd-b6b873e65126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89324.9, 19141.05, 19141.05, 127607, 108466)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train, val, and test sets partitioning ratio\n",
    "127607*0.7, 127607*0.15, 127607*0.15, 89325+19141+19141, 89325+19141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9684093-9035-4eb8-aafb-73ba315e205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign training, validation, and test sets\n",
    "input_train_df = user_input_df[0:89325]\n",
    "output_train_df = user_output_df[0:89325]\n",
    "input_valid_df = user_input_df[89325:108466]\n",
    "output_valid_df = user_output_df[89325:108466]\n",
    "input_test_df = user_input_df[108466:127607]\n",
    "output_test_df = user_output_df[108466:127607]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98e6c5e0-8736-4ead-aab8-08719ddcc93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89325, 89325)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_train_df), len(output_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52e927bf-242f-4dd4-b6dc-f28a8d6be893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19141, 19141)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_valid_df), len(output_valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db2f3f28-28a6-4e62-a375-d023bdea5e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19141, 19141)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_test_df), len(output_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87e48124-ee44-4515-a780-1e7a2edfd288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EN tokenizer\n",
    "eng_tokenizer = get_tokenizer('spacy', language = 'en_core_web_lg')\n",
    "\n",
    "def hindi_tokenizer(data):\n",
    "    factory = IndicNormalizerFactory()\n",
    "    normalizer = factory.get_normalizer('hi',remove_nuktas = True)\n",
    "    text = normalizer.normalize(data)\n",
    "    words = indic_tokenize.trivial_tokenize(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2df1ce72-35c6-4637-9526-61e6b4e44baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a Vocabulary with only words that occur a minimum of 1 times.\n",
    "\n",
    "def build_bivocab(filedata, tokenizer):\n",
    "    counter = Counter()\n",
    "    for string_ in filedata:\n",
    "        #print(string_)\n",
    "        counter.update(tokenizer(str(string_)))\n",
    "    #print(counter)\n",
    "    return vocab(counter, specials = ['<unk>', '<pad>', '<bos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80276529-7629-43fe-a641-d75b1753f94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_vocab = build_bivocab(input_train_df, eng_tokenizer)\n",
    "eng_vocab.set_default_index(eng_vocab['<unk>'])\n",
    "eng_vocab[' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1883c60-e413-43f7-bc6c-c7283a18b4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi_vocab = build_bivocab(output_train_df,hindi_tokenizer)\n",
    "hindi_vocab.set_default_index(hindi_vocab['<unk>'])\n",
    "hindi_vocab['<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e3af5df-4ce1-49a4-8781-12bc252b8774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3273\n"
     ]
    }
   ],
   "source": [
    "print(eng_vocab['politics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43c0f7e9-f8d1-4008-8c33-186d9ccbf18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2477"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eng_vocab.parameters()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "222531d6-aadc-4b95-bc74-530315c5ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(filepath_input, filepath_output):\n",
    "    raw_eng_iter = iter(filepath_input)\n",
    "    raw_hindi_iter = iter(filepath_output)\n",
    "    data = []\n",
    "    for (raw_eng, raw_hindi) in zip(raw_eng_iter, raw_hindi_iter):\n",
    "        eng_tensor_ = torch.tensor([eng_vocab[token] for token in eng_tokenizer(str(raw_eng))],dtype = torch.long)\n",
    "        hindi_tensor_ = torch.tensor([hindi_vocab[token] for token in hindi_tokenizer(str(raw_hindi))],dtype = torch.long)\n",
    "        data.append((eng_tensor_, hindi_tensor_))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2cc61b-7115-4fea-b62a-8ffcb34a3ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = process_df(input_train_df, output_train_df)\n",
    "val_df = process_df(input_valid_df, output_valid_df)\n",
    "test_df = process_df(input_test_df, output_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5babcfe0-055b-4e8c-9030-f3b0b54c0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of the EN-HI train set:\", len(train_df))\n",
    "print(\"Size of the EN-HI val set:\", len(val_df))\n",
    "print(\"Size of the EN-HI test set:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e9e0c0-6ca0-4380-9fe0-f199689ea712",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "PAD_IDX = eng_vocab['<pad>']\n",
    "BOS_IDX = eng_vocab['<bos>']\n",
    "EOS_IDX = eng_vocab['<eos>']\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa8f68-075c-47f4-8d1b-d55402a310e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "    eng_batch, hindi_batch = [], []\n",
    "    for (eng_item, hindi_item) in data_batch:\n",
    "        eng_batch.append(torch.cat([torch.tensor([BOS_IDX]), eng_item, torch.tensor([EOS_IDX])], dim = 0))\n",
    "        hindi_batch.append(torch.cat([torch.tensor([BOS_IDX]), hindi_item, torch.tensor([EOS_IDX])], dim = 0))\n",
    "    eng_batch = pad_sequence(eng_batch, padding_value = PAD_IDX)\n",
    "    hindi_batch = pad_sequence(hindi_batch, padding_value = PAD_IDX)\n",
    "    return eng_batch, hindi_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02385490-5fab-4f42-b901-74ad9fe7775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = DataLoader(train_df, batch_size = BATCH_SIZE, shuffle = True, collate_fn = generate_batch)\n",
    "valid_iter = DataLoader(val_df, batch_size = BATCH_SIZE, shuffle = True, collate_fn = generate_batch)\n",
    "test_iter = DataLoader(test_df, batch_size = BATCH_SIZE, shuffle = True, collate_fn = generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8c0f3-3f9a-4694-b7a8-cf1ae711f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ea5d0-e754-41a6-b65a-29396e1592f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for (idx, batch) in enumerate(valid_iter):\n",
    "    #print(idx)\n",
    "    #print(batch[0])\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53cd93-be23-4a4e-8ed0-3d13c4275397",
   "metadata": {},
   "source": [
    "## Modelling (standard transformer model based on https://arxiv.org/abs/1409.0473)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b66da2-4471-43d8-948e-8eac22c65cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers:int, num_decoder_layers:int, emb_size:int, src_vocab_size:int, tgt_vocab_size:int, dim_feedforward:int = 512, dropout:float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        encoder_layer = TransformerEncoderLayer(d_model = emb_size, nhead = NHEAD, dim_feedforward = dim_feedforward)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers = num_encoder_layers)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model = emb_size, nhead = NHEAD, dim_feedforward = dim_feedforward)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers = num_decoder_layers)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout = dropout)\n",
    "        \n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask:Tensor, tgt_mask:Tensor, src_padding_mask:Tensor, tgt_padding_mask:Tensor, memory_key_padding_mask:Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n",
    "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "    \n",
    "    def encode(self, src:Tensor, src_mask:Tensor):\n",
    "        return self.transformer_encoder(self.positional_encoding(self.src_tok_emb(src)), src_mask)\n",
    "    \n",
    "    def decode(self, tgt:Tensor, memory:Tensor, tgt_mask:Tensor):\n",
    "        return self.transformer_decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask)   \n",
    "    \n",
    "    \n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self,vocab_size:int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "    def forward(self, tokens:Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size:int, dropout, maxlen:int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(-torch.arange(0,emb_size, 2) * math.log(10000)/emb_size)\n",
    "        pos = torch.arange(0,maxlen).reshape(maxlen,1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:,0::2] = torch.sin(pos*den)\n",
    "        pos_embedding[:,1::2] = torch.cos(pos*den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding',pos_embedding)\n",
    "    \n",
    "    def forward(self, token_embedding:Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0),:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c46ba8-2c91-4766-ae62-97ce5a8393c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz,sz), device = DEVICE)) == 1).transpose(0,1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src,tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "    \n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device = DEVICE).type(torch.bool)\n",
    "    \n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0,1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0,1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e0a0a4-7f7c-4cfa-af58-32326cd5bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_VOCAB_SIZE = len(eng_vocab)\n",
    "HI_VOCAB_SIZE = len(hindi_vocab)\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 32\n",
    "NUM_ENCODER_LAYERS = 6\n",
    "NUM_DECODER_LAYERS = 6\n",
    "NUM_EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c76bb9e-3e91-4800-84da-16ddb69b0fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, EN_VOCAB_SIZE, HI_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim()>1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr = 0.0001, betas = (0.9,0.98), eps = 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f98cf5-b9f1-48d8-a23a-0e503e1f41aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_iter, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for idx, (eng,hindi) in enumerate(train_iter):\n",
    "        eng = eng.to(DEVICE)\n",
    "        hindi = hindi.to(DEVICE)\n",
    "        hindi_input = hindi[:-1,:]\n",
    "        eng_mask, hindi_mask, eng_padding_mask, hindi_padding_mask = create_mask(eng, hindi_input)\n",
    "        logits = model(eng, hindi_input, eng_mask, hindi_mask, eng_padding_mask, hindi_padding_mask, eng_padding_mask)\n",
    "        optimizer.zero_grad()\n",
    "        hindi_out = hindi[1:,:]\n",
    "        loss = loss_fn(logits.reshape(-1,logits.shape[-1]), hindi_out.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "    torch.save(model,'s2sattn_model_500.pth')\n",
    "    return losses/len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d37512-9dc8-429a-9aa5-7eea8780972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    for idx, (eng, hindi) in (enumerate(valid_iter)):\n",
    "        eng = eng.to(DEVICE)\n",
    "        hindi = hindi.to(DEVICE)\n",
    "        hindi_input = hindi[:-1,:]\n",
    "        eng_mask, hindi_mask, eng_padding_mask, hindi_padding_mask = create_mask(eng, hindi_input)\n",
    "        logits = model(eng, hindi_input, eng_mask, hindi_mask, eng_padding_mask, hindi_padding_mask, eng_padding_mask)\n",
    "        hindi_out = hindi[1:,:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), hindi_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "    return losses/ len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35d306d2-b3c6-469a-bb93-2046978af3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.97 GiB (GPU 0; 15.89 GiB total capacity; 14.52 GiB already allocated; 527.88 MiB free; 14.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, NUM_EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      5\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(transformer, valid_iter)\n",
      "Cell \u001b[0;32mIn[36], line 13\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_iter, optimizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m hindi_out \u001b[38;5;241m=\u001b[39m hindi[\u001b[38;5;241m1\u001b[39m:,:]\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), hindi_out\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.97 GiB (GPU 0; 15.89 GiB total capacity; 14.52 GiB already allocated; 527.88 MiB free; 14.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(transformer, train_iter, optimizer)\n",
    "    end_time = time.time()\n",
    "    val_loss = evaluate(transformer, valid_iter)\n",
    "    print((f\"Epoch : {epoch}, Train loss: {train_loss:.3f}, Val Loss: {val_loss:.3f},\" \n",
    "           f\"Epoch Time= {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7bbc62-5b38-4c03-a866-bd0df0870f12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
